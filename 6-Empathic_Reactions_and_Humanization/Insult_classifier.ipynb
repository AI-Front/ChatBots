{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Insult_classifier.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMrgnPHOOxJNK7bUNnyh4za",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI-Front/ChatBots/blob/master/6-Empathic_Reactions_and_Humanization/Insult_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN9UopqHhKmR",
        "colab_type": "text"
      },
      "source": [
        "# Simple pretrained insult classifier with BERT "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaG8YJYyhWUR",
        "colab_type": "text"
      },
      "source": [
        "This tutorial describes how to make a simple classifier of insultive messages within DeepPavlov framework. \n",
        "\n",
        "Also see [TowardsDataSciense tutorial](https://towardsdatascience.com/the-bert-based-text-classification-models-of-deeppavlov-a85892f14d61)\n",
        "\n",
        "In this tutorial we will learn\n",
        "\n",
        "0. How to install and interact with model\n",
        "0. Its architecture\n",
        "0. Simple skill examples for your bot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPXCw8sXi0jA",
        "colab_type": "text"
      },
      "source": [
        "# How to install and interact with model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxtvsMlUf5H8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3672848-960b-4f95-cc25-7ce3a17c1e57"
      },
      "source": [
        "!pip install deeppavlov\n",
        "!python -m deeppavlov install rusentiment_bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deeppavlov\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/25/7c97c184d13c579ddc4550f58c0c4fc480a6122393008c21c4e4fad6d64d/deeppavlov-0.11.0-py3-none-any.whl (847kB)\n",
            "\u001b[K     |████████████████████████████████| 849kB 6.9MB/s \n",
            "\u001b[?25hCollecting pymorphy2==0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.4MB/s \n",
            "\u001b[?25hCollecting uvicorn==0.11.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/11f4b4bf3963ead6de570feeae49eeced02f6768cf1f68e16f4b16d3b0aa/uvicorn-0.11.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (2.10.0)\n",
            "Collecting sacremoses==0.0.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 22.6MB/s \n",
            "\u001b[?25hCollecting Cython==0.29.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/d1/4d3f8a7a920e805488a966cc6ab55c978a712240f584445d703c08b9f405/Cython-0.29.14-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 47.6MB/s \n",
            "\u001b[?25hCollecting pydantic==1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/24/e78cf017628e7eaed20cb040999b1ecc69f872da53dfd0d9aed40c0fa5f1/pydantic-1.3-cp36-cp36m-manylinux2010_x86_64.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 27.4MB/s \n",
            "\u001b[?25hCollecting pyopenssl==19.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d348039954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm==4.41.1 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (4.41.1)\n",
            "Collecting pandas==0.25.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/3f/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 49.9MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 46.2MB/s \n",
            "\u001b[?25hCollecting pytelegrambotapi==3.6.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/ab/99c606f69fcda57e35788b913dd34c9d9acb48dd26349141b3855dcf6351/pyTelegramBotAPI-3.6.7.tar.gz (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.1MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml==0.15.100\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/9f/83bb34eaf84032b0b54fcc4a6aff1858572d279d65a301c7ae875f523df5/ruamel.yaml-0.15.100-cp36-cp36m-manylinux1_x86_64.whl (656kB)\n",
            "\u001b[K     |████████████████████████████████| 665kB 43.8MB/s \n",
            "\u001b[?25hCollecting aio-pika==6.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/07/196a4115cbef31fa0c3dabdea146f02dffe5e49998341d20dbe2278953bc/aio_pika-6.4.1-py3-none-any.whl (40kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.4MB/s \n",
            "\u001b[?25hCollecting rusenttokenize==0.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/25/4c/a2f00be5def774a3df2e5387145f1cb54e324607ec4a7e23f573645946e7/rusenttokenize-0.0.5-py3-none-any.whl\n",
            "Collecting scikit-learn==0.21.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/04/49633f490f726da6e454fddc8e938bbb5bfed2001681118d3814c219b723/scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 44.6MB/s \n",
            "\u001b[?25hCollecting pytz==2019.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/73/fe30c2daaaa0713420d0382b16fbb761409f532c56bdcc514bf7b6262bb6/pytz-2019.1-py2.py3-none-any.whl (510kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 47.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (1.4.1)\n",
            "Collecting numpy==1.18.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/45f71bd24f4e37629e9db5fb75caab919507deae6a5a257f9e4685a5f931/numpy-1.18.0-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 1.3MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/9b/358faaff410f65a4ad159275e897b5956dcb20576c5b8e764b971c1634d7/pymorphy2_dicts_ru-2.4.404381.4453942-py2.py3-none-any.whl (8.0MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0MB 43.4MB/s \n",
            "\u001b[?25hCollecting overrides==2.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/98/2430afd204c48ac0a529d439d7e22df8fa603c668d03456b5947cb59ec36/overrides-2.7.0.tar.gz\n",
            "Collecting requests==2.22.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.2MB/s \n",
            "\u001b[?25hCollecting fastapi==0.47.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/a7/4804d7abf8a1544d079d50650af872387154ebdac5bd07d54b2e60e2b334/fastapi-0.47.1-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: click==7.1.2 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (7.1.2)\n",
            "Collecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 48.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
            "Collecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.2MB/s \n",
            "\u001b[?25hCollecting httptools==0.0.13; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"pypy\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/03/215969db11abe8741e9c266a4cbe803a372bd86dd35fa0084c4df6d4bd00/httptools-0.0.13.tar.gz (104kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 53.6MB/s \n",
            "\u001b[?25hCollecting websockets==8.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.5MB/s \n",
            "\u001b[?25hCollecting uvloop>=0.14.0; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"pypy\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/48/586225bbb02d3bdca475b17e4be5ce5b3f09da2d6979f359916c1592a687/uvloop-0.14.0-cp36-cp36m-manylinux2010_x86_64.whl (3.9MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 43.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py==2.10.0->deeppavlov) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.35->deeppavlov) (0.16.0)\n",
            "Requirement already satisfied: dataclasses>=0.6; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from pydantic==1.3->deeppavlov) (0.7)\n",
            "Collecting cryptography>=2.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/04/686efee2dcdd25aecf357992e7d9362f443eb182ecd623f882bc9f7a6bba/cryptography-2.9.2-cp35-abi3-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 46.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas==0.25.3->deeppavlov) (2.8.1)\n",
            "Collecting yarl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/8f/0209fc5d975f839344c33c822ff2f7ef80f6b1e984673a5a68f960bfa583/yarl-1.4.2-cp36-cp36m-manylinux1_x86_64.whl (252kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 55.8MB/s \n",
            "\u001b[?25hCollecting aiormq<4,>=3.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/90/e8089608c2fcf75d6a8ff805611a1e31ed7cc523f5eafe244a2fb3dd75b8/aiormq-3.2.2-py3-none-any.whl\n",
            "Collecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
            "Collecting starlette<=0.12.9,>=0.12.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/95/2220fe5bf287e693a6430d8ee36c681b0157035b7249ec08f8fb36319d16/starlette-0.12.9.tar.gz (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (1.14.0)\n",
            "Collecting multidict>=4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/95/f50352b5366e7d579e8b99631680a9e32e1b22adfa1629a8f23b1d22d5e2/multidict-4.7.6-cp36-cp36m-manylinux1_x86_64.whl (148kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 53.9MB/s \n",
            "\u001b[?25hCollecting pamqp==2.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/56/afa06143361e640c9159d828dadc95fc9195c52c95b4a97d136617b0166d/pamqp-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (2.20)\n",
            "Building wheels for collected packages: sacremoses, nltk, pytelegrambotapi, overrides, httptools, starlette\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=5cf2462d83c946bc518039d749c5b6ba43a0ea2fc4ed826c4e8a92d8c50dc46f\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449907 sha256=d406d2b8d250c7d53ece6dcc656ac0a11ba04e687c3ceb7b01d43df7824b84e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytelegrambotapi: filename=pyTelegramBotAPI-3.6.7-cp36-none-any.whl size=47178 sha256=13a06a0f8d7b921d0789bf05f91819b27bfa3bae879aba5b4bc6c51ba49149b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/40/18/8a34153f95ef0dc19e3954898e5a5079244b76a8afdd7d0ec5\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.7.0-cp36-none-any.whl size=5600 sha256=216044dd7f2eb7e625f8c0934dd53dfc8cdf09ea447b2450306426625d4aac43\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/7c/ef/80508418b67d87371c5b3de49e03eb22ee7c1d19affb5099f8\n",
            "  Building wheel for httptools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for httptools: filename=httptools-0.0.13-cp36-cp36m-linux_x86_64.whl size=212525 sha256=4c9a1091483fcd6b6d7abf3e5d7e8f3bafaed65569f252c9df9dc3600fa75c9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/3e/2e/013f99b42efc25cf3589730cf380738e46b1e5edaf2f78d525\n",
            "  Building wheel for starlette (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for starlette: filename=starlette-0.12.9-cp36-none-any.whl size=57245 sha256=bdb1071e9762662417598c25a6974275c2f30bfde4feb38d284d2bc7c8811df4\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/51/5b/3828d52e185cafad941c4291b6f70894d0794be28c70addae5\n",
            "Successfully built sacremoses nltk pytelegrambotapi overrides httptools starlette\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.0.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: dawg-python, pymorphy2-dicts, pymorphy2, h11, httptools, websockets, uvloop, uvicorn, sacremoses, Cython, pydantic, cryptography, pyopenssl, pytz, numpy, pandas, nltk, idna, requests, pytelegrambotapi, ruamel.yaml, multidict, yarl, pamqp, aiormq, aio-pika, rusenttokenize, scikit-learn, pymorphy2-dicts-ru, overrides, starlette, fastapi, deeppavlov\n",
            "  Found existing installation: Cython 0.29.20\n",
            "    Uninstalling Cython-0.29.20:\n",
            "      Successfully uninstalled Cython-0.29.20\n",
            "  Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: pandas 1.0.5\n",
            "    Uninstalling pandas-1.0.5:\n",
            "      Successfully uninstalled pandas-1.0.5\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed Cython-0.29.14 aio-pika-6.4.1 aiormq-3.2.2 cryptography-2.9.2 dawg-python-0.7.2 deeppavlov-0.11.0 fastapi-0.47.1 h11-0.9.0 httptools-0.0.13 idna-2.8 multidict-4.7.6 nltk-3.4.5 numpy-1.18.0 overrides-2.7.0 pamqp-2.3.0 pandas-0.25.3 pydantic-1.3 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pymorphy2-dicts-ru-2.4.404381.4453942 pyopenssl-19.1.0 pytelegrambotapi-3.6.7 pytz-2019.1 requests-2.22.0 ruamel.yaml-0.15.100 rusenttokenize-0.0.5 sacremoses-0.0.35 scikit-learn-0.21.2 starlette-0.12.9 uvicorn-0.11.1 uvloop-0.14.0 websockets-8.1 yarl-1.4.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "idna",
                  "numpy",
                  "pandas",
                  "pytz",
                  "requests"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2020-07-14 13:56:54.407 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'rusentiment_bert' as '/usr/local/lib/python3.6/dist-packages/deeppavlov/configs/classifiers/rusentiment_bert.json'\n",
            "Collecting tensorflow==1.15.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/d9/fd234c7bf68638423fb8e7f44af7fcfce3bcaf416b51e6d902391e47ec43/tensorflow-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (110.5MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5MB 89kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.34.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.9.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 31.2MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 50.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.30.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.12.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.2.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.18.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.2) (49.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=37ae01b0f5de08d3e35de17fc8714be327f1b544a5b661abd9fb26feed5df6ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, gast, tensorflow\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n",
            "Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n",
            "  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-jdx4magm\n",
            "  Running command git clone -q https://github.com/deepmipt/bert.git /tmp/pip-req-build-jdx4magm\n",
            "Building wheels for collected packages: bert-dp\n",
            "  Building wheel for bert-dp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-dp: filename=bert_dp-1.0-cp36-none-any.whl size=23581 sha256=5848447301f24ae88c4aa2cef98b31a83bfaa2dcae1ab550bc1fe347e8cd10ce\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jkzqlyk2/wheels/1e/41/94/886107eaf932532594886fd8bfc9cb9d4db632e94add49d326\n",
            "Successfully built bert-dp\n",
            "Installing collected packages: bert-dp\n",
            "Successfully installed bert-dp-1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7uQZucXjN_6",
        "colab_type": "text"
      },
      "source": [
        "Importing the model and downloading BERT as the source of sentence embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlLDVPDhgAPv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9fe0035e-0839-4974-b953-d01685bf2d8f"
      },
      "source": [
        "from deeppavlov import build_model, configs\n",
        "\n",
        "model = build_model(configs.classifiers.insults_kaggle_bert, download=True)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-07-14 13:57:49.207 INFO in 'deeppavlov.core.data.utils'['utils'] at line 94: Downloading from http://files.deeppavlov.ai/deeppavlov_data/bert/cased_L-12_H-768_A-12.zip to /root/.deeppavlov/downloads/cased_L-12_H-768_A-12.zip\n",
            "100%|██████████| 404M/404M [02:12<00:00, 3.06MB/s]\n",
            "2020-07-14 14:00:01.325 INFO in 'deeppavlov.core.data.utils'['utils'] at line 269: Extracting /root/.deeppavlov/downloads/cased_L-12_H-768_A-12.zip archive into /root/.deeppavlov/downloads/bert_models\n",
            "2020-07-14 14:00:05.631 INFO in 'deeppavlov.core.data.utils'['utils'] at line 94: Downloading from http://files.deeppavlov.ai/deeppavlov_data/classifiers/insults_kaggle_v3.tar.gz to /root/.deeppavlov/models/insults_kaggle_v3.tar.gz\n",
            "100%|██████████| 804M/804M [03:21<00:00, 3.98MB/s]\n",
            "2020-07-14 14:03:27.736 INFO in 'deeppavlov.core.data.utils'['utils'] at line 269: Extracting /root/.deeppavlov/models/insults_kaggle_v3.tar.gz archive into /root/.deeppavlov/models/classifiers\n",
            "2020-07-14 14:03:37.630 INFO in 'deeppavlov.core.data.utils'['utils'] at line 94: Downloading from http://files.deeppavlov.ai/datasets/insults_data.tar.gz to /root/.deeppavlov/insults_data.tar.gz\n",
            "100%|██████████| 682k/682k [00:00<00:00, 2.00MB/s]\n",
            "2020-07-14 14:03:38.78 INFO in 'deeppavlov.core.data.utils'['utils'] at line 269: Extracting /root/.deeppavlov/insults_data.tar.gz archive into /root/.deeppavlov/downloads\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-07-14 14:03:41.134 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/classifiers/insults_kaggle_v3/classes.dict]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_classifier.py:84: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_classifier.py:161: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert_dp/modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:234: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:127: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:127: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_classifier.py:92: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/models/bert/bert_classifier.py:97: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-07-14 14:03:58.909 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /root/.deeppavlov/models/classifiers/insults_kaggle_v3/model]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from /root/.deeppavlov/models/classifiers/insults_kaggle_v3/model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KEANKmyjZhl",
        "colab_type": "text"
      },
      "source": [
        "Interacting with pretrained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "womXkHjPgqVU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e5102fb-c8e4-4bfe-e4cc-18a85ea74288"
      },
      "source": [
        "model(['hey, how are you?', 'You are so dumb!'])\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Not Insult', 'Insult']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QTDMJBhk-3D",
        "colab_type": "text"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43jE7iPqmfKi",
        "colab_type": "text"
      },
      "source": [
        "The model is based on original BERT model - trained using a next sentence prediction (NSP) objective using the [CLS] token as a sequence approximate: \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIPxgQvwmCti",
        "colab_type": "text"
      },
      "source": [
        "![bert_arch.png](https://github.com/AI-Front/ChatBots/blob/master/6-Empathic_Reactions_and_Humanization/bert.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym8x_RpwqEHG",
        "colab_type": "text"
      },
      "source": [
        "A sequence classifier is trained on [CLS] token with a sequence classification/regression head on top (a linear layer on top of the pooled output) \n",
        "\n",
        "Original architecture - [HuggingFace BertForSequenceClassification class](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertForSequenceClassification)\n",
        "\n",
        "DeepPavlov provides an easy-to-use solution for one of the most popular tasks in NLP — the text classification problem — by using pretrained BERT. One can use several pretrained English or multilingual models. For the text classification case, the final hidden state (i.e., Transformer output) corresponding to the [CLS] token is used as the aggregate sequence representation for classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWnV21xIqPgY",
        "colab_type": "text"
      },
      "source": [
        "![bert_arch.png](https://github.com/AI-Front/ChatBots/blob/master/6-Empathic_Reactions_and_Humanization/unnamed.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phRwddtYlo-Q",
        "colab_type": "text"
      },
      "source": [
        "The model was trained on the [Detecting Insults in Social Commentary dataset](https://www.kaggle.com/c/detecting-insults-in-social-commentary) that contains binary classification task for detecting insults for participants of conversation.\n",
        "\n",
        " Train, valid and test division is the same as for the Kaggle challenge.\n",
        "\n",
        " The mothod results in 92% ROC-AUC on various types of embeddings - Reddit embeddings, original BERT and Conversational BERT. \n",
        "\n",
        "See the [docs](http://docs.deeppavlov.ai/en/master/features/models/classifiers.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4FPcKx-lDUU",
        "colab_type": "text"
      },
      "source": [
        "# Simple Skill example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHXeuCl_lINj",
        "colab_type": "text"
      },
      "source": [
        "Here we have our simple skill class (described in [previous lessons](https://github.com/AI-Front/ChatBots/tree/master/3-Basic_Architecture)) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRgriAHgg9If",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "replics = [\"Hey, don't be so rude\", 'Am I a joke to you?', 'Sorry, bots are not working under pressure', \"Remember 'Toxic' by Britney Spears? Cause that's about you\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjnUC-9RgwKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InsultSkill():\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(InsultSkill, self).__init__(*args, **kwargs)\n",
        "        self.reply = [\"Hey, don't be so rude\", 'Am I a joke to you?', 'Sorry, bots are not working under pressure', \"Remember 'Toxic' by Britney Spears? Cause that's about you\"]\n",
        "        self.insult = model\n",
        " \n",
        "    def __call__(self, text, states=None, **kwargs):\n",
        "        res = self.get_answer(states)\n",
        "        return res\n",
        "\n",
        "    def fit(self, *args, **kwargs):\n",
        "        \"\"\"Fit skill function.\"\"\"\n",
        "        raise NotImplemented\n",
        "\n",
        "    def classify(self, text, states):\n",
        "        sentiment = self.check_skill(text)\n",
        "        if sentiment:\n",
        "            res = self.get_answer(states)\n",
        "            return {'name': 'InsultSkill', 'answer': res}\n",
        "        return {}\n",
        "  \n",
        "    def get_answer(self, states=None):\n",
        "        #this is skill confidence score - as we use classifier, we will use its class proability score\n",
        "        score = 1.0\n",
        "        res = {\n",
        "                \"res_utterances\": [[random.choice(self.reply)]],\n",
        "                \"states\": states,\n",
        "                \"confidence_batch\": [[score]]}\n",
        "        return res\n",
        "\n",
        "    def check_skill(self, text):\n",
        "        #you can eigher put insult classifier here\n",
        "        res = self.insult([text])[0]\n",
        "        if res=='Insult':\n",
        "            return True\n",
        "        return False"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFZzRJuam1-A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5d7b9907-8152-4a76-eb5a-35b998a06e04"
      },
      "source": [
        "skill = InsultSkill()\n",
        "text_message = 'I hate you'\n",
        "skill.classify(text_message, {})"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': {'confidence_batch': [[1.0]],\n",
              "  'res_utterances': [['Am I a joke to you?']],\n",
              "  'states': {}},\n",
              " 'name': 'InsultSkill'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}